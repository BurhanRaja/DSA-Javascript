NLP (Natural Language Processing) is the cross-section of linguistics and computer science.

NLP is subfield of AI that focuses on a computer to understand, interpret and generate human language. 
This is the core of ChatGPT.

Computers know numbers and vectors.

7 steps of NLP :-

1. Segmentation :-

Split the sentence into individual words Or we can say it tokenize them in OpenAI pricing model context, i.e., 
1000 token is 750 words.

2. Tokenization :-

The words are converted to a standard format like converted to lowercase.

3. Stop Words :-

It removes the words that do not contribute to the meaning of the sentences, to reduce the amount of data that needs
to be processed.

4. Stemming :-

It reduces the words to its base form or stemm in order to make the model's ability to generalize and recognize patterns. 
This might include algorithms that removes suffixs and leave the word with its core meaning.

5. limitization :-

Similar to stemming. It reduces certain words to its dictionary form.

6. Speech Tagging :-

It indicates the individual role of the word like verb, noun etc.

7. Named Entity Recognition :-

In this final step, it identifies the named entity, a proper noun such as a specific people organizations or locations.


After these 7 steps, it tries to learn.
This understanding of the NLP model is typically represented as numbers and vector-based data structures that can be easily 
processed by model.

Speech tagging example :-

[
[0 0 0 1] # pronoun
[0 1 0 0] # auxilary verb
[0 0 1 0] # verb
[0 0 0 0] # proper noun
[1 0 0 0] # stop word
[0 1 0 1] # noun
[1 0 0 0] # stop word
]
 

GPT (Generative Pretrained Model)

It contains 2 things an Encoder and a Decoder

The above 7 steps of NLP happens in Encoder, so the output of the Encoder is a vector-based representation of the 
sentence that captures the meaning and structure of the sentence in a compact and efficient form

In decoder we use sequence to sequence transformation. It like sending a sequence of sentence or question getting back
another sequence like another sentence or answer to our question.

Transformers are the new Machine Learning technology, they were introduced in 2017 in the paper called 
"Attention Is All You Need".

Transformers work based on self-attention mechanism that focuses on the most relevant parts of 
the input when generating the ouput by calculating how much attention is needed on each vector should
recieve based on the other vectors in inputs. This is the reason they so good at summarizing content 
because tranformers understands context.

Transformers allow the model to effectively process input sequence of any length in a 
parallel and efficient manner. It vastly outperforms recurrent neural networks and 
convolutional neural network.

Open AI got creative and used variety of different machine learning techniques to get the best results.

An initial model was trained with Supervised fine tuning. Here lot of prompts were cerated and 
a human described the desired input and the language model is looking to understand what's going
on?

It also trained a Reward model with a similar process. A propmt with four different options were 
given and the labeler ranks the output from the best to worst. This is fed to the model and then 
it tries to maximize the reward using reiforcement learning.  


 

